Dear Kristen,

We regret to inform you that we are unable to include your submission, Quality Metrics of Test Suites in Test-Driven Designed Applications, in the Research Track
of ICSME 2014. In total, we received 267 abstracts and 210 full paper submissions. Only 40 of these
papers were accepted, yielding an acceptance rate of 19%.

All papers went through a rigorous reviewing process in which they received at least three independent
reviews, followed by discussions among the assigned reviewers, and, finally, a discussion open to the
entire program committee. The program committee selected papers for inclusion in the program
without targeting any quota or acceptance rate: merits and weaknesses of each paper were discussed
independently of other papers taken into consideration.

Attached to this email, you will find the reviews of your paper. We hope that you will find them helpful in
the revision of your paper for other ICSME tracks or other venues. In particular, we hope that you will
consider submitting your work to some of the other tracks of ICSME 2014: the Early Research
Achievement (ERA) Track, the Industry Track, the Tool Demo Track, and the Doctoral Symposium;
especially if the reviewers suggest you to do so. Note that the deadline for abstract submissions for
these tracks is June 23 (AoE), the deadline for full submissions (4 pages) is June 30 (with grace period til
July 7). More information on these can be found at http://www.icsme.org/call-for-papers

You may also consider submitting your work to one of the workshops co-located with ICSME 2014.
More information on those can be found at http://www.icsme.org/2014

We sincerely hope that you will attend ICSME 2014 in Victoria and look forward to seeing you there
September 28 - October 3, 2014.

Thank you for submitting your paper to ICSME 2014


Best regards,
Leon Moonen and Lori Pollock
ICSME 2014 Program Co-Chairs


----------------------- REVIEW 1 ---------------------
PAPER: 22
TITLE: Quality Metrics of Test Suites in Test-Driven Designed Applications
AUTHORS: Hessah Alkaoud and Kristen Walcott-Justice


----------- REVIEW -----------
This paper presents a study that compares statement coverage, branch coverage and mutation coverage (using Jacoco and MAJOR) between Java projects that have been developed with TDD and without TDD.

This paper is well written and the type of experiment they describe would be interesting if more TDD projects would have been used.  Why did you only use 3? Could you not find more TDD projects?

Moreover, I have the following concerns:


* section IV.B, ``We suspect that these higher coverage metrics are due to the fact that these four <non-TDD> applications are under active development in large communities that have high standards for acceptance of new features.''  So you suspect that if non-TDD projects have high quality standards, their testing activities will be done well independent whether TDD is used or not.... So maybe we can suspect the same about the selected TDD projects which are only 3?


* section IV.B (below Fig 6), ``Comparing the two groups of applications in terms of the branch coverage metric, the p-value was 0.059 which is not significant, but is approaching statistically significant results. Therefore, we can speculate that with more TDD programs we might be able to conclude a statistically significant difference between the application groups, where the TDD group has higher branch coverage than the non-TDD group''


* Section V, ``In TDD, we found a trend between test-LOC per Source-
Line of Code with high branch coverage and statement coverage.'' I am not sure that with 3 TDD projects that are not even that large (1 only has 554 LOC) we can find a trend......


* Section V,  ``In other words, as more tests are written in TDD, branch and the statement coverage improve.'' Using a strict test-first approach in TDD this is almost true per definition?


----------------------- REVIEW 2 ---------------------
PAPER: 22
TITLE: Quality Metrics of Test Suites in Test-Driven Designed Applications
AUTHORS: Hessah Alkaoud and Kristen Walcott-Justice


----------- REVIEW -----------
This paper empirically analyzed the three quality metrics (statement coverage, branch coverage, and mutation score) in test suites of the applications developed using Test Driven-Development (TDD), and compared them to the results from the same experiments with test suites of the applications developed using non-TDD. The empirical results on 11 open source applications showed that the test suites developed in TDD exhibit higher quality in terms of the three quality metrics than the test suites developed in non-TDD. Also, the authors revealed that mutation scores have statistically meaningful correlations to both statement coverage and branch coverage.

As TDD practices become more popular in developing software, the question related the quality of test suites in TDD applications is interesting. Especially, the authors’ question “whether TDD actually promotes high quality test suites or not” is very welcomed. However, I have one fundamental concern that the quality of test suites could be more directly influenced by test case generation techniques (i.e. coverage-directed, random, and manual, etc.) rather than the methodologies (i.e. TDD vs. non-TDD). In other words, the test quality is not only related with adapting TDD but also related with the specific methods for generating test cases. For example, if the test suites of non-TDD applications were generated by mutation testing tools, the mutation scores for the test suites would be surely high. I I would like to suggest that more explanation is added to how they controlled the test suites for each application.

Motivation of this paper is interesting and the experimental results have relevant contributions. On the other hand, the paper may needs more organized.
- In Section Ⅰ, the introduction seems  too long to effectively show the motivation and summarize contributions. Although the authors listed the three main contributions in the last paragraph of Section 1, the first contribution is not clearly differentiated with the second contribution.
- Section Ⅱ is too descriptive and redundant. For example, a half page is used to describe the statement coverage and the branch coverage while they are very common so that authors could give summarized sentences with references. The sentence “Refactoring is a process of altering … functionality [14]” is the same sentence in the introduction section. Instead, the detailed analysis of the experimental results with the reasons of the outliers in Section Ⅵ would help make the paper stronger.
- In Section Ⅵ, the authors wrote “we answer three key questions: Q1, … Q2, … and Q3 … ” in the first paragraph, while they wrote “In our research, we answer two key questions: 1), … 2)” in the fourth paragraph. This would make confusion for readers to understand this paper.


[Minor comments]
P3 C1 code examples: use figure numbers and titles to easily refer them
P4 C2 L10: “Thus, we also will also” -> “Thus, we will also”


----------------------- REVIEW 3 ---------------------
PAPER: 22
TITLE: Quality Metrics of Test Suites in Test-Driven Designed Applications
AUTHORS: Hessah Alkaoud and Kristen Walcott-Justice


----------- REVIEW -----------
Summary and Evaluation:
This paper presents a comparative evaluation between TDD and non-TDD developed applications, measuring test suite quality in terms of code coverage metrics and mutation score. The results show high correlation between the structural coverage and mutation score for TDD applications and more variation for the non-TDD applications.

I really like the idea behind this paper, to compare the quality of test suites of programs developed using TDD and non-TDD approaches. The primary contribution of this work is the evaluation, which is in my opinion too weak at this stage to justify acceptance. The rest of this review provides some concrete suggestions for improvement.

Estimating test suite quality using structural coverage and mutation coverage is reasonable, though I would have been interested to see another metric along the lines of defect density. However, given that open source systems were being used, measuring this may not be straightforward, or measures may not be reliable. The study design I suggest later in this review may help along those lines.

A flaw with the evaluation is that it's comparing apples (TDD applications) and oranges (non-TDD applications), and in very small numbers. While this is good to get a feel for how the test metrics relate, it does not tell us a whole lot. I see two potential directions to improve this evaluation, beyond expanding the metrics. The first is to greatly increase the number of applications evaluated. The second is to facilitate a more direct comparison of the techniques. Along the lines of the latter suggestion, I'd like to see a comparison of two identical applications created using each approach, since this would seem like a more fair comparison, and perhaps even allow for measuring defect density if an oracle test suite was developed. A study like this seems like it could be executed in the classroom with students. Say individual or groups of students are instructed to create an application that does X, and half use TDD while the other half use whatever methodology they want. T!
 hen, for application Y, the roles reverse. This would allow for a more direct comparison of the metrics across multiple versions of similar applications.

The statistics used in the results are a little off. Using parametric statistical tests like t-test and Person's correlation assume normality of the data. Without knowing the normality of the population from which the data points were sampled, this does not seem appropriate. It also seems inappropriate to measure normality based on sample sizes of 3 and 8. Instead, stick with the non-parametric tests to be safe. Further, an alpha value of 0.05 seemed to be assumed throughout the paper but was not stated in some cases; statistical significance is with respect to an alpha.

Smaller comments:

I: The first two points in the contributions at the end of this section overlap a lot

II: The paper does a nice job of introducing the concepts of coverage metrics and mutation scores, but the background section reads more like a textbook and could be greatly reduced in size while still getting across the main points.
